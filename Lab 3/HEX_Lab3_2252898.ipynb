{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Language Model and Application for Spelling Error Correction\n",
        "## Objective: Develop a simple English syntax error correction program\n",
        "\n",
        "## Home Exercise:\n",
        "\n",
        "a) Improve the model by using interpolation smoothing with the \"Stupid Backoff\" method (Brants et al., 2007).\n",
        "\n",
        "b) Compare with the results from In Class Exercise.\n",
        "\n",
        "c) Use the newly built model to generate the next words for a given word sequence.\n",
        "\n",
        "d) Combine with a function that calculates the distance between words to predict the correct word for a misspelled word position. (from difflib import get_close_matches)"
      ],
      "metadata": {
        "id": "T-Gpi02VqMOE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzJ2VJMFMwBX"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY7-XncrRIPK",
        "outputId": "1f732827-f30f-40ac-fd77-63cfcd9e69a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgblFgcsGtPI",
        "outputId": "9e9e49a4-904e-47ee-ac13-c73f50fd1d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import contractions\n",
        "from collections import Counter\n",
        "from difflib import get_close_matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo1UXH9JN-9s"
      },
      "source": [
        "## Data Downloading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HjDEZRnVM3Cv"
      },
      "outputs": [],
      "source": [
        "with open('/content/tedtalk.txt') as file:\n",
        "  docs = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXec_f3WPfTn"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set()     # Number of unique word\n",
        "token_count = 0   # Number of token\n",
        "\n",
        "# Lower case character\n",
        "def text_lowercase(text):\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Split into sentences\n",
        "def sent_tokenize(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "# Removing contractions and Keep number and letters\n",
        "def remove_punctuation(text):\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r'[^a-z0-9\\' ]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "# Tokenize the text into words\n",
        "def tokenize(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Data preprocessing\n",
        "def preprocess_text(text, is_addvocab=False):\n",
        "    global token_count\n",
        "    text = text_lowercase(text)\n",
        "    sentences = sent_tokenize(text)\n",
        "    processed_sentences = []\n",
        "    for sentence in sentences:\n",
        "      sentence = remove_punctuation(sentence)\n",
        "      sentence = tokenize(sentence)\n",
        "      if is_addvocab:\n",
        "        token_count += len(sentence)\n",
        "        for token in sentence:\n",
        "          vocab.add(token)\n",
        "      processed_sentences.append(sentence)\n",
        "\n",
        "    return processed_sentences"
      ],
      "metadata": {
        "id": "pIdCt-JBc2gq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BEM_WqqvPex8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8fd294-d80d-491e-b7a9-96fa327c5f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 68782\n",
            "Number of tokens: 7396888\n"
          ]
        }
      ],
      "source": [
        "preprocess_data = preprocess_text(docs,True)\n",
        "vocab.add('<s>')\n",
        "vocab.add('</s>')\n",
        "print(\"Number of unique words:\", len(vocab))\n",
        "print(\"Number of tokens:\", token_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKmTJGkkWtsw"
      },
      "source": [
        "## a) Improve the model by using interpolation smoothing with the \"Stupid Backoff\" method (Brants et al., 2007)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_word(word, cutoff=0.6):\n",
        "    matches = get_close_matches(word, vocab, n=5, cutoff=0.7)\n",
        "    return matches[0] if matches else word\n"
      ],
      "metadata": {
        "id": "5G6etmzbaieF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_XL1lOa6RJH9"
      },
      "outputs": [],
      "source": [
        "class NgramModel:\n",
        "    def __init__(self, n, v, token_count):\n",
        "      self.n = n\n",
        "      self.ngrams = Counter()\n",
        "      self.context = Counter()\n",
        "      self.vocab = v\n",
        "      self.vocab_size = len(v)\n",
        "      self.token_count = token_count\n",
        "      self.discount_factor = 0.4\n",
        "      # if self.n == 1:\n",
        "      #   self.ngrams[(tuple(['<s>']))] += 1\n",
        "      #   self.ngrams[(tuple(['</s>']))] += 1\n",
        "\n",
        "\n",
        "    def padding_both_ends(self, tokens):\n",
        "      if self.n == 1:\n",
        "        return tokens\n",
        "      else:\n",
        "        return ['<s>'] * (self.n - 1) + tokens + ['</s>'] * (self.n - 1) # adding <s> <s> and </s> </s> for trigram\n",
        "\n",
        "\n",
        "    # Train the model by counting n-grams and contexts\n",
        "    def train(self, corpus):\n",
        "      for tokens in corpus:\n",
        "        tokens = self.padding_both_ends(tokens)\n",
        "        for i in range(len(tokens) - self.n + 1):\n",
        "          ngram = tuple(tokens[i:i+self.n]) # Create n-gram\n",
        "          self.ngrams[ngram] += 1\n",
        "          self.context[ngram[:-1]] += 1\n",
        "\n",
        "\n",
        "    # Calculate the probability of an n-gram using Laplace smoothing\n",
        "    def laplace_smoothing(self, ngram):\n",
        "      if self.n != 1:\n",
        "        return (self.ngrams[ngram] + 1) / (self.context[ngram[:-1]] + self.vocab_size)\n",
        "      else:\n",
        "        return (self.ngrams[ngram] + 1) / (self.token_count + self.vocab_size)\n",
        "\n",
        "\n",
        "    # Calculate the probability of an n-gram using Stupid backoff\n",
        "    def stupid_backoff(self, ngram):\n",
        "      for k in range(self.n, 0, -1):\n",
        "          backoff_ngram = ngram[-k:]\n",
        "          if self.ngrams[backoff_ngram] > 0:\n",
        "            if len(backoff_ngram) != 1:\n",
        "              prob = self.ngrams[backoff_ngram] / self.context[backoff_ngram[:-1]]\n",
        "              return prob\n",
        "            else:\n",
        "              # print(backoff_ngram, 'here')\n",
        "              prob = self.ngrams[backoff_ngram] / self.token_count\n",
        "              return prob\n",
        "      if len(ngram) > 1:\n",
        "          # If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram\n",
        "          return self.discount_factor * self.stupid_backoff(ngram[1:])\n",
        "      return 1 / (self.token_count + self.vocab_size) # Return a small probability for UNK\n",
        "\n",
        "\n",
        "    # Calculate the probability of a given sentence\n",
        "    def sentence_probability(self, sentence):\n",
        "      laplace_prob = 1\n",
        "      backoff_prob = 1\n",
        "      tokens = preprocess_text(sentence)[0]\n",
        "      tokens = self.padding_both_ends(tokens)\n",
        "      for i in range(len(tokens) - self.n + 1):\n",
        "          ngram = tuple(tokens[i:i+self.n])\n",
        "\n",
        "          laplace = self.laplace_smoothing(ngram)\n",
        "          backoff = self.stupid_backoff(ngram)\n",
        "\n",
        "          laplace_prob *= laplace\n",
        "          backoff_prob *= backoff\n",
        "      return laplace_prob, backoff_prob\n",
        "\n",
        "\n",
        "    # Calculate the perplexity of a given sentence\n",
        "    def sentence_perplexity(self, sentence):\n",
        "      tokens = preprocess_text(sentence)[0]\n",
        "      tokens = self.padding_both_ends(tokens)\n",
        "      N = len(tokens)\n",
        "      laplace_prob, backoff_prob = self.sentence_probability(sentence)\n",
        "      return laplace_prob ** (-1 / N), backoff_prob ** (-1 / N)\n",
        "\n",
        "\n",
        "    # Generate a next word for a given word sequence.\n",
        "    def generate_next_word(self,sentence):\n",
        "      tokens = preprocess_text(sentence)[0]\n",
        "\n",
        "      max_prob = 0\n",
        "      best_word = None\n",
        "\n",
        "      print(f\"Given Sequence: {sentence}\")\n",
        "      print(f\"Candidate Words and Probabilities:\")\n",
        "\n",
        "      for word in self.vocab:\n",
        "        if word == '<s>':\n",
        "          continue\n",
        "\n",
        "        # Create a candidate bgram and select the one with highest probability as predicted netxt word\n",
        "        candidate_ngram = tuple(tokens[-(self.n - 1):] + [word])\n",
        "        prob = self.stupid_backoff(candidate_ngram)\n",
        "\n",
        "        if prob > max_prob:\n",
        "            max_prob = prob\n",
        "            best_word = word\n",
        "\n",
        "      print(f\"\\nBest Next Word: {best_word} (Probability: {max_prob})\")\n",
        "      return best_word\n",
        "\n",
        "\n",
        "    # Generate a full sentence from initial sentence\n",
        "    def generate_sentence(self, initial_sequence, max_length=10):\n",
        "      tokens = preprocess_text(initial_sequence)[0]\n",
        "\n",
        "      print(f\"Initial Sequence: {' '.join(tokens)}\\nGenerating...\\n\")\n",
        "\n",
        "      for _ in range(max_length):\n",
        "          next_word = self.generate_next_word(' '.join(tokens[-(self.n - 1):]))\n",
        "\n",
        "          if next_word =='</s>':\n",
        "              break  # End the sentence if the predicted word is </s>\n",
        "\n",
        "          tokens.append(next_word)\n",
        "\n",
        "          print(f\"Current Sentence: {' '.join(tokens)}\")\n",
        "\n",
        "      generated_sentence = ' '.join(tokens)\n",
        "      print(f\"\\nGenerated Sentence: {generated_sentence}\")\n",
        "      return generated_sentence\n",
        "\n",
        "\n",
        "     # Generate a full sentence from misspelled initial sentence\n",
        "    def generate_sentence_for_misspelled(self, initial_sequence, max_length=10):\n",
        "      tokens = preprocess_text(initial_sequence)[0]\n",
        "      for i in range(len(tokens)):\n",
        "        corrected_word = correct_word(word=tokens[i])\n",
        "        if corrected_word != tokens[i]:\n",
        "            tokens[i] = corrected_word\n",
        "\n",
        "      print(f\"Initial Sequence: {' '.join(tokens)}\\nGenerating...\\n\")\n",
        "\n",
        "      for _ in range(max_length):\n",
        "          next_word = self.generate_next_word(' '.join(tokens[-(self.n - 1):]))\n",
        "\n",
        "          if next_word =='</s>':\n",
        "              break  # End the sentence if the predicted word is </s>\n",
        "\n",
        "          tokens.append(next_word)\n",
        "\n",
        "          print(f\"Current Sentence: {' '.join(tokens)}\")\n",
        "\n",
        "      generated_sentence = ' '.join(tokens)\n",
        "      print(f\"\\nGenerated Sentence: {generated_sentence}\")\n",
        "      return generated_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Rp9IFB1u6bSd"
      },
      "outputs": [],
      "source": [
        "# Train unigram, bigram, and trigram model\n",
        "unigram_model = NgramModel(n=1, v=vocab, token_count=token_count)\n",
        "bigram_model = NgramModel(n=2, v=vocab, token_count=token_count)\n",
        "trigram_model = NgramModel(n=3, v=vocab, token_count=token_count)\n",
        "\n",
        "unigram_model.train(preprocess_data)\n",
        "bigram_model.train(preprocess_data)\n",
        "trigram_model.train(preprocess_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model.ngrams.update(unigram_model.ngrams)\n",
        "bigram_model.context.update(unigram_model.context)\n",
        "\n",
        "trigram_model.ngrams.update(unigram_model.ngrams)\n",
        "trigram_model.ngrams.update(bigram_model.ngrams)\n",
        "trigram_model.context.update(unigram_model.context)\n",
        "trigram_model.context.update(bigram_model.context)"
      ],
      "metadata": {
        "id": "HlGB-aCoF2z_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekXhQvgh6Vse"
      },
      "source": [
        "## b) Compare with the results from In Class Exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Wt3u_hvhG4mQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f34309-70e6-46c7-d399-6bb0e77b9a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Model:\n",
            "  Probability of correct sentence:\n",
            "    Base on Laplace smooth = 2.493751526229549e-25\n",
            "    Base on Stupid backoff = 2.700615182081296e-25\n",
            "  Perplexity of correct sentence:\n",
            "    Base on Laplace smooth = 541.6053929661736\n",
            "    Base on Stupid backoff = 536.8308646005801\n",
            "  Probability of incorrect sentence:\n",
            "    Base on Laplace smooth = 2.493751526229549e-25\n",
            "    Base on Stupid backoff = 2.7006151820812953e-25\n",
            "  Perplexity of incorrect sentence:\n",
            "    Base on Laplace smooth = 541.6053929661736\n",
            "    Base on Stupid backoff = 536.8308646005802\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Bigram Model:\n",
            "  Probability of correct sentence:\n",
            "    Base on Laplace smooth = 5.595492433883201e-25\n",
            "    Base on Stupid backoff = 8.607043216510398e-17\n",
            "  Perplexity of correct sentence:\n",
            "    Base on Laplace smooth = 160.2293133509159\n",
            "    Base on Stupid backoff = 28.871398401479873\n",
            "  Probability of incorrect sentence:\n",
            "    Base on Laplace smooth = 2.36665923693406e-30\n",
            "    Base on Stupid backoff = 2.386778267586458e-21\n",
            "  Perplexity of incorrect sentence:\n",
            "    Base on Laplace smooth = 493.4695827711494\n",
            "    Base on Stupid backoff = 74.94529213954281\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Trigram Model:\n",
            "  Probability of correct sentence:\n",
            "    Base on Laplace smooth = 2.3805849313469555e-34\n",
            "    Base on Stupid backoff = 3.368247915883469e-14\n",
            "  Perplexity of correct sentence:\n",
            "    Base on Laplace smooth = 385.8415092294\n",
            "    Base on Stupid backoff = 10.873103814907292\n",
            "  Probability of incorrect sentence:\n",
            "    Base on Laplace smooth = 8.632231554283533e-42\n",
            "    Base on Stupid backoff = 4.356552340672235e-19\n",
            "  Perplexity of incorrect sentence:\n",
            "    Base on Laplace smooth = 1441.3178426165796\n",
            "    Base on Stupid backoff = 25.844826456298108\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Sample sentences for evaluation\n",
        "correct_sentence = \"I want to give a speech at Ted Talk.\"\n",
        "incorrect_sentence = \"I want give a speech to at Ted Talk.\"\n",
        "\n",
        "# Calculate probabilities and perplexities of each sentence\n",
        "for model, name in zip([unigram_model, bigram_model, trigram_model], [\"Unigram\", \"Bigram\", \"Trigram\"]):\n",
        "    print(f\"{name} Model:\")\n",
        "    print(f\"  Probability of correct sentence:\")\n",
        "    print(f\"    Base on Laplace smooth = {model.sentence_probability(correct_sentence)[0]}\")\n",
        "    print(f\"    Base on Stupid backoff = {model.sentence_probability(correct_sentence)[1]}\")\n",
        "    print(f\"  Perplexity of correct sentence:\")\n",
        "    print(f\"    Base on Laplace smooth = {model.sentence_perplexity(correct_sentence)[0]}\")\n",
        "    print(f\"    Base on Stupid backoff = {model.sentence_perplexity(correct_sentence)[1]}\")\n",
        "\n",
        "    print(f\"  Probability of incorrect sentence:\")\n",
        "    print(f\"    Base on Laplace smooth = {model.sentence_probability(incorrect_sentence)[0]}\")\n",
        "    print(f\"    Base on Stupid backoff = {model.sentence_probability(incorrect_sentence)[1]}\")\n",
        "    print(f\"  Perplexity of incorrect sentence:\")\n",
        "    print(f\"    Base on Laplace smooth = {model.sentence_perplexity(incorrect_sentence)[0]}\")\n",
        "    print(f\"    Base on Stupid backoff = {model.sentence_perplexity(incorrect_sentence)[1]}\")\n",
        "\n",
        "    print(100*'-')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze the results\n",
        "From the result, we can conclude that the Stupid backoff method performs better than Laplace smoothing method. The reason is that the Laplace smoothing treat all ngram the same way by adding 1 to the count of ngram, it leading to reduce mass probability when encounter unseen ngrams, while the Stuipid backoff consider backoff to the lower n-gram when the count of higher one is zero\n"
      ],
      "metadata": {
        "id": "cmJt04YPn2Kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c) Use the newly built model to generate the next words for a given word sequence."
      ],
      "metadata": {
        "id": "sMtsg43zYG8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intial_sentence = 'I want to make'\n",
        "trigram_complete_sentence = trigram_model.generate_sentence(intial_sentence,20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqilPptqJ9VJ",
        "outputId": "80f8d51e-503f-4cad-d0cd-a1f224a6a187"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Sequence: i want to make\n",
            "Generating...\n",
            "\n",
            "Given Sequence: to make\n",
            "Candidate Words and Probabilities:\n",
            "\n",
            "Best Next Word: a (Probability: 0.13801244902339557)\n",
            "Current Sentence: i want to make a\n",
            "Given Sequence: make a\n",
            "Candidate Words and Probabilities:\n",
            "\n",
            "Best Next Word: difference (Probability: 0.12779973649538867)\n",
            "Current Sentence: i want to make a difference\n",
            "Given Sequence: a difference\n",
            "Candidate Words and Probabilities:\n",
            "\n",
            "Best Next Word: </s> (Probability: 0.3743016759776536)\n",
            "\n",
            "Generated Sentence: i want to make a difference\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d) Combine with a function that calculates the distance between words to predict the correct word for a misspelled word position. (from difflib import get_close_matches)"
      ],
      "metadata": {
        "id": "39TsLoNnYLq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "misspelled_sentence = 'I waant to mnake'\n",
        "trigram_complete_sentence = trigram_model.generate_sentence_for_misspelled(misspelled_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMDHzEP3kDcl",
        "outputId": "29d1af45-c5c0-45c3-adc6-913d68e04e29"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Sequence: i want to make\n",
            "Generating...\n",
            "\n",
            "Given Sequence: to make\n",
            "Candidate Words and Probabilities:\n",
            "\n",
            "Best Next Word: a (Probability: 0.13801244902339557)\n",
            "Current Sentence: i want to make a\n",
            "Given Sequence: make a\n",
            "Candidate Words and Probabilities:\n",
            "\n",
            "Best Next Word: difference (Probability: 0.12779973649538867)\n",
            "Current Sentence: i want to make a difference\n",
            "Given Sequence: a difference\n",
            "Candidate Words and Probabilities:\n",
            "\n",
            "Best Next Word: </s> (Probability: 0.3743016759776536)\n",
            "\n",
            "Generated Sentence: i want to make a difference\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}