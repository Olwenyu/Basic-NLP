{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121416743,"sourceType":"kernelVersion"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Homework Lab 2: Text Preprocessing with Vietnamese\n**Overview:** In this exercise, we will build a text preprocessing program for Vietnamese.","metadata":{"id":"kFD7DVZ-xKdT"}},{"cell_type":"markdown","source":"Import the necessary libraries. Note that we are using the underthesea library for Vietnamese tokenization. To install it, follow the instructions below. ([link](https://github.com/undertheseanlp/underthesea))","metadata":{"id":"XOAeiqdrxKdt"}},{"cell_type":"code","source":"%pip install underthesea","metadata":{"id":"cFMu692jw_fo","outputId":"6ca5cf70-2925-4243-89df-f12c58a61568","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:56:26.849575Z","iopub.execute_input":"2025-01-22T13:56:26.849848Z","iopub.status.idle":"2025-01-22T13:56:34.359493Z","shell.execute_reply.started":"2025-01-22T13:56:26.849823Z","shell.execute_reply":"2025-01-22T13:56:34.358416Z"}},"outputs":[{"name":"stdout","text":"Collecting underthesea\n  Downloading underthesea-6.8.4-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\nCollecting python-crfsuite>=0.9.6 (from underthesea)\n  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.2.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.32.3)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.4.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.2)\nCollecting underthesea-core==1.0.4 (from underthesea)\n  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.12.14)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.13.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->underthesea) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn->underthesea) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\nDownloading underthesea-6.8.4-py3-none-any.whl (20.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\nSuccessfully installed python-crfsuite-0.9.11 underthesea-6.8.4 underthesea-core-1.0.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os,glob\nimport codecs\nimport sys\nimport re\nfrom underthesea import word_tokenize","metadata":{"id":"RrFQ_Ht_xKdu","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:56:34.360565Z","iopub.execute_input":"2025-01-22T13:56:34.360909Z","iopub.status.idle":"2025-01-22T13:56:48.032079Z","shell.execute_reply.started":"2025-01-22T13:56:34.360858Z","shell.execute_reply":"2025-01-22T13:56:48.031025Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Question 1: Create a Corpus and Survey the Data\n\nThe data in this section is partially extracted from the [VNTC](https://github.com/duyvuleo/VNTC) dataset. VNTC is a Vietnamese news dataset covering various topics. In this section, we will only process the science topic from VNTC. We will create a corpus from both the train and test directories. Complete the following program:\n\n- Write `sentences_list` to a file named `dataset_name.txt`, with each element as a document on a separate line.\n- Check how many documents are in the corpus.\n","metadata":{"id":"hC27lBQZxKdw"}},{"cell_type":"code","source":"# Dataset https://www.kaggle.com/code/tientrungcao/vntc-text-classification/output\n%cd /kaggle/input/vntc-text-classification\ndataset_name = \"VNTC_khoahoc\"\npath = ['./Train_Full/', './Test_Full/']\n\nif os.listdir(path[0]) == os.listdir(path[1]):\n    folder_list = [os.listdir(path[0]), os.listdir(path[1])]\n    print(\"train labels = test labels\")\nelse:\n    print(\"train labels differ from test labels\")\n\ndoc_num = 0\nsentences_list = []\nmeta_data_list = []\nfor i in range(2):\n    folder_path = path[i] + 'Khoa hoc'\n    for file_name in glob.glob(os.path.join(folder_path, '*.txt')):\n        # Read the file content into f\n        f = codecs.open(file_name, 'br')\n        # Convert the data to UTF-16 format for Vietnamese text\n        file_content = (f.read().decode(\"utf-16\")).replace(\"\\r\\n\", \" \")\n        sentences_list.append(file_content.strip())\n        f.close\n        # Count the number of documents\n        doc_num += 1\n\n#### YOUR CODE HERE ####\n# Number of documents\nprint('Number of documents =', doc_num)\n\n%cd /kaggle/working/\n\n# Write to VNTC_khoahoc.txt file\nfile = open(dataset_name + '.txt', 'w')\nfor sentence in sentences_list:\n    file.write(sentence + '\\n')\nfile.close()\n\n#### END YOUR CODE #####","metadata":{"id":"GyNKT8wAxKdx","outputId":"b176fc73-912d-419d-84f0-a3bf9b002a8f","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:56:48.033020Z","iopub.execute_input":"2025-01-22T13:56:48.033440Z","iopub.status.idle":"2025-01-22T13:57:07.368241Z","shell.execute_reply.started":"2025-01-22T13:56:48.033414Z","shell.execute_reply":"2025-01-22T13:57:07.367042Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vntc-text-classification\ntrain labels = test labels\nNumber of documents = 3916\n/kaggle/working\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Question 2: Write Preprocessing Functions\n\n\n\n\n\n","metadata":{"id":"xj4srX53wyQv"}},{"cell_type":"markdown","source":"### Question 2.1: Write a Function to Clean Text\nHint:\n- The text should only retain the following characters: aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0-9(),!?\\'\\\n- Then trim the whitespace in the input text.","metadata":{"id":"3KXHcDpuxKd0"}},{"cell_type":"code","source":"def clean_str(string):\n    #### YOUR CODE HERE ####\n    allowed_chars =  r\"a-zA-Z0-9\\(\\),!?\\'\\\\àÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬđĐèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆìÌỉỈĩĨíÍịỊòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰỳỲỷỶỹỸýÝỵỴ\"\n    cleaned_text = re.sub(f\"[^ {allowed_chars}]\", \" \", string)\n    cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n    return cleaned_text.strip()\n    #### END YOUR CODE #####","metadata":{"id":"k8hIglDXxKd0","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:57:07.369401Z","iopub.execute_input":"2025-01-22T13:57:07.369744Z","iopub.status.idle":"2025-01-22T13:57:07.374513Z","shell.execute_reply.started":"2025-01-22T13:57:07.369715Z","shell.execute_reply":"2025-01-22T13:57:07.373568Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Question 2.2: Write a Function to Convert Text to Lowercase","metadata":{"id":"9KfXstqAxKd1"}},{"cell_type":"code","source":"def text_lowercase(string):\n    #### YOUR CODE HERE ####\n    return string.lower()\n    #### END YOUR CODE #####","metadata":{"id":"KRwgVjxhxKd1","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:57:07.375580Z","iopub.execute_input":"2025-01-22T13:57:07.375929Z","iopub.status.idle":"2025-01-22T13:57:07.569961Z","shell.execute_reply.started":"2025-01-22T13:57:07.375901Z","shell.execute_reply":"2025-01-22T13:57:07.568821Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Question 2.3: Tokenize Words\nHint: Use the `word_tokenize()` function imported above with two parameters: `strings` and `format=\"text\"`.\n","metadata":{"id":"rYM_GO_5xKd2"}},{"cell_type":"code","source":"def tokenize(strings):\n    #### YOUR CODE HERE ####\n    return word_tokenize(strings, format=\"text\")\n    #### END YOUR CODE #####","metadata":{"id":"pty34NwyxKd2","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:57:07.572399Z","iopub.execute_input":"2025-01-22T13:57:07.572725Z","iopub.status.idle":"2025-01-22T13:57:07.591165Z","shell.execute_reply.started":"2025-01-22T13:57:07.572697Z","shell.execute_reply":"2025-01-22T13:57:07.590114Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Question 2.4: Remove Stop Words\nTo remove stop words, we use a list of Vietnamese stop words stored in the file `./vietnamese-stopwords.txt`. Complete the following program:\n- Check each word in the text (`strings`). If a word is not in the stop words list, add it to `doc_words`.\n","metadata":{"id":"-gQGmL4gxKd2"}},{"cell_type":"code","source":"# Save stop words to list\n!git clone https://github.com/stopwords/vietnamese-stopwords.git\nwith open('./vietnamese-stopwords/vietnamese-stopwords.txt', 'r', encoding='utf-8') as f:\n    stop_words = f.read().splitlines()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:57:07.592424Z","iopub.execute_input":"2025-01-22T13:57:07.592780Z","iopub.status.idle":"2025-01-22T13:57:08.308114Z","shell.execute_reply.started":"2025-01-22T13:57:07.592751Z","shell.execute_reply":"2025-01-22T13:57:08.306751Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'vietnamese-stopwords'...\nremote: Enumerating objects: 95, done.\u001b[K\nremote: Counting objects: 100% (14/14), done.\u001b[K\nremote: Compressing objects: 100% (10/10), done.\u001b[K\nremote: Total 95 (delta 3), reused 0 (delta 0), pack-reused 81 (from 1)\u001b[K\nReceiving objects: 100% (95/95), 40.25 KiB | 3.10 MiB/s, done.\nResolving deltas: 100% (31/31), done.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def remove_stopwords(strings):\n    #### YOUR CODE HERE ####\n    words = strings.split()\n    # Remove any words appear in the stop words list\n    doc_words = [word for word in words if word not in stop_words]\n    return doc_words\n    #### END YOUR CODE #####","metadata":{"id":"aqStv2rPxKd3","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:57:08.309454Z","iopub.execute_input":"2025-01-22T13:57:08.309770Z","iopub.status.idle":"2025-01-22T13:57:08.315038Z","shell.execute_reply.started":"2025-01-22T13:57:08.309741Z","shell.execute_reply":"2025-01-22T13:57:08.314012Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Question 2.5: Build a Preprocessing Function\nHint: Call the functions `clean_str`, `text_lowercase`, `tokenize`, and `remove_stopwords` in order, then return the result from the function.\n","metadata":{"id":"jUNOKigIxKd4"}},{"cell_type":"code","source":"def text_preprocessing(strings):\n    #### YOUR CODE HERE ####\n    text = clean_str(strings)\n    text = text_lowercase(text)\n    text = tokenize(text)\n    text = remove_stopwords(text)\n    return ' '.join(text)\n    #### END YOUR CODE #####","metadata":{"id":"_vd-el91xKd_","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:57:08.316398Z","iopub.execute_input":"2025-01-22T13:57:08.316821Z","iopub.status.idle":"2025-01-22T13:57:08.342738Z","shell.execute_reply.started":"2025-01-22T13:57:08.316779Z","shell.execute_reply":"2025-01-22T13:57:08.341475Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Question 3: Perform Preprocessing\nNow, we will read the corpus from the file created in Question 1. After that, we will call the preprocessing function for each document in the corpus.\n\nHint: Call the `text_preprocessing()` function with `doc_content` as the input parameter and save the result in the variable `temp1`.\n","metadata":{"id":"1BGOqa1mxKeA"}},{"cell_type":"code","source":"#### YOUR CODE HERE ####\nclean_docs = []\nwith open(dataset_name + '.txt', 'r') as f:\n    for doc_content in f:\n        # Processing each document\n        temp1 = text_preprocessing(doc_content)\n        clean_docs.append(temp1)\n#### END YOUR CODE #####\n\nprint(\"Length of clean_docs = \", len(clean_docs))\nprint('Clean_docs[0]:\\n' + clean_docs[0])","metadata":{"id":"OfyI9hIDwyQz","outputId":"401ac561-1acc-42c4-e0d3-fdeacc7829b5","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:57:08.343983Z","iopub.execute_input":"2025-01-22T13:57:08.344323Z","iopub.status.idle":"2025-01-22T13:59:50.420177Z","shell.execute_reply.started":"2025-01-22T13:57:08.344281Z","shell.execute_reply":"2025-01-22T13:59:50.419003Z"}},"outputs":[{"name":"stdout","text":"Length of clean_docs =  3916\nClean_docs[0]:\nchiến_thắng hay_là chết olympic cổ_đại ? 564 công_nguyên , lực_sĩ arrichion phigaleia , vô_địch olympic môn pankration kết_hợp đấm bốc vật trao vòng_nguyệt quế vinh_quang tử_vong tranh_tài giành vương_miện olympic 3 , arrichion đối_thủ bóp_cổ không_thể thoát gọng kìm_kinh_hoàng , arrichion tóm cổ_chân đối_thủ vặn gãy đau_đớn , đối_phương khuất_phục , cổ arrichion thắt chặt , ta công_bố chiến_thắng , arrichion trút hơi thở cuối_cùng mặc_dù chết arrichion xảy tình_huống bi_kịch , câu_chuyện vận_động_viên olympic từ_bỏ mạng sống giành chiến_thắng hề hiếm hy_lạp cổ_đại môn pankration bạo_lực , bóp_cổ , bẻ ngón đấm hạ_bộ phép , vận_động_viên tổn_thương nặng_nề hầu_hết chết vết_thương trận đấu kết_thúc có_điều kỳ_olympic cổ_đại dấy niềm khát_khao chiến_thắng mãnh_liệt vận_động_viên bỏ_mạng thất_bại ? truyền_thuyết , olympic bắt_đầu 776 công_nguyên , môn thi duy_nhất chạy_đua nước_rút 192 mét diễn khu đền thờ_thần zeus olympia hồi , vận_động_viên chạy koroibos đích trước_tiên trở_thành vô_địch olympic đầu_tiên lịch_sử trò_chơi lễ_hội tôn_giáo diễn 4 , tôn_vinh_vị thần_zeus sự_kiện kéo_dài 1 100 hoàng_đế theodosius hủy 393 công_nguyên sự_kiện lạc_lõng hội thời_gian , luật_lệ bổ_sung tất_cả 10 bộ_môn , diễn đường đua , sân đấu lưng ngựa , kéo_dài 5 ngày_càng olympic phát_triển uy_tín danh_vọng , đại_hội thể_thao thành_phố ban_ngày ngôi_sao bầu_trời ấm mặt_trời , kỳ thi_đấu vĩ_đại olympic games , nhà_thơ hy_lạp pindar nhận_xét thế_kỷ 5 công_nguyên olympic trở_thành sự_kiện toàn_cầu , mở_cửa miễn_phí tất_cả đàn_ông hy_lạp , tiếp công_dân la_mã thu_hút đấu_sĩ tây ban_nha biển đen 4 , sứ_giả đi khắp hy_lạp loan báo lệnh ngừng_bắn lưu_thông đường bang , điều_kiện thuận_lợi đi_lại vận_động_viên khán_giả hầu_hết , lệnh ngừng_bắn giám_sát kỹ_lưỡng 420 công_nguyên , sparta cấm tham_dự đại_hội tấn_công thị_trấn elis thời_kỳ ngừng_bắn 364 công_nguyên , arcadia elean ẩu_đả lẫn khu_vực linh_thiêng , môn điền_kinh diễn quyết_liệt vận_động_viên olympia hoàn_toàn tự_nguyện chi_phí quan_điểm vận_động_viên nghiệp_dư chiến_đấu vinh_quang chiến_thắng sai_lầm mặc_dù giải_thưởng duy_nhất trao olympia vòng_nguyệt_quế , ta chiến_thắng phần_thưởng hậu_hĩnh trở quê_hương 600 công_nguyên , vinh_quang phần_thưởng tiền_mặt 500 đồng_drachma , tương_đương 300 000 usd ngày_nay vận_động_viên món quà cao_quý , bữa miễn_phí tôn_vinh , trọng_dụng lợi_lộc , danh_vọng nịnh_đầm tôn_thờ , thổi bùng_niềm khát_khao giành chiến_thắng giá tham_gia , thủ_đoạn mục_đích tuy_vậy , thời , vận_động_viên tuyên_thệ tuân luật_lệ xuất_phát sai quy_định hành_vi bất_hợp_pháp phạt đánh roi công_chúng tước quyền thi_đấu thế_kỷ 4 , vận_động_viên lừa_dối , gian_lận , hối_lộ phạt tiền tiền dựng tượng zeus dọc lối sân_vận_động chứng_thực mãi_mãi vết nhơ xung_đột lý_tưởng cao_thượng olympic hành_vi chính_trị thương_mại hóa đặc_điểm thế_vận_hội cổ_điển hiện_đại yếu_tố thống_nhất xuyên suốt kỳ_olympic thuở sơ_khai nằm theo_đuổi hy_lạp gọi_là đỉnh vinh_quang gói gọn khẩu_hiệu , , ( citius , altius , fortius )\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Question 4: Save Preprocessed Data\nHint: Save the preprocessed data to a file named `dataset_name + '.clean.txt'`, where each document is written on a separate line.\n","metadata":{"id":"SFhai6BwxKeB"}},{"cell_type":"code","source":"#### YOUR CODE HERE ####\nfile = open(dataset_name + '.clean.txt', 'w')\nfor docs in clean_docs:\n    file.write(docs + '\\n')\nfile.close()\n#### YOUR CODE HERE ####","metadata":{"id":"xfHmSiRrxKeB","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:59:50.421263Z","iopub.execute_input":"2025-01-22T13:59:50.421627Z","iopub.status.idle":"2025-01-22T13:59:50.466927Z","shell.execute_reply.started":"2025-01-22T13:59:50.421597Z","shell.execute_reply":"2025-01-22T13:59:50.465634Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink('VNTC_khoahoc.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:59:50.468187Z","iopub.execute_input":"2025-01-22T13:59:50.468607Z","iopub.status.idle":"2025-01-22T13:59:50.475905Z","shell.execute_reply.started":"2025-01-22T13:59:50.468567Z","shell.execute_reply":"2025-01-22T13:59:50.474992Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/VNTC_khoahoc.txt","text/html":"<a href='VNTC_khoahoc.txt' target='_blank'>VNTC_khoahoc.txt</a><br>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"FileLink('VNTC_khoahoc.clean.txt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:59:50.477029Z","iopub.execute_input":"2025-01-22T13:59:50.477319Z","iopub.status.idle":"2025-01-22T13:59:50.497768Z","shell.execute_reply.started":"2025-01-22T13:59:50.477295Z","shell.execute_reply":"2025-01-22T13:59:50.496741Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/VNTC_khoahoc.clean.txt","text/html":"<a href='VNTC_khoahoc.clean.txt' target='_blank'>VNTC_khoahoc.clean.txt</a><br>"},"metadata":{}}],"execution_count":13}]}